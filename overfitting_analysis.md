# PH√ÇN T√çCH NGUY C∆† OVERFITTING - CREDIT CARD FRAUD DETECTION

## üö® NH·ªÆNG D·∫§U HI·ªÜU OVERFITTING NGHI√äM TR·ªåNG

### 1. **IMBALANCED DATA C·ª∞C K·ª≤ NGHI√äM TR·ªåNG**
- **T·ªâ l·ªá**: 99.83% giao d·ªãch h·ª£p l·ªá vs 0.17% gian l·∫≠n
- **V·∫•n ƒë·ªÅ**: M√¥ h√¨nh c√≥ th·ªÉ ƒë·∫°t 99.8% accuracy ch·ªâ b·∫±ng c√°ch d·ª± ƒëo√°n T·∫§T C·∫¢ l√† "kh√¥ng gian l·∫≠n"
- **Nguy c∆°**: M√¥ h√¨nh "h·ªçc thu·ªôc l√≤ng" pattern c·ªßa class majority

### 2. **THI·∫æU VALIDATION SET**
```python
# Ch·ªâ c√≥ train/test split
X_train, X_test, y_train, y_test = train_test_split(...)
```
- **V·∫•n ƒë·ªÅ**: Kh√¥ng c√≥ validation set ƒë·ªÉ monitor overfitting
- **H·∫≠u qu·∫£**: Kh√¥ng bi·∫øt khi n√†o m√¥ h√¨nh b·∫Øt ƒë·∫ßu overfit

### 3. **S·ª¨ D·ª§NG ACCURACY L√ÄM METRIC CH√çNH**
- **Trong imbalanced data**: Accuracy l√† metric g√¢y hi·ªÉu l·∫ßm
- **V√≠ d·ª•**: Model d·ª± ƒëo√°n t·∫•t c·∫£ = 0 ‚Üí Accuracy = 99.8%
- **Th·ª±c t·∫ø**: Model ho√†n to√†n v√¥ d·ª•ng cho fraud detection

### 4. **KH√îNG C√ì REGULARIZATION**
```python
# Decision Tree kh√¥ng c√≥ max_depth, min_samples_split
dtree = DecisionTreeClassifier(criterion='entropy', random_state=0)

# Logistic Regression kh√¥ng c√≥ regularization
model = LogisticRegression()
```

### 5. **KH√îNG C√ì CROSS-VALIDATION**
- Ch·ªâ test tr√™n 1 split duy nh·∫•t
- K·∫øt qu·∫£ c√≥ th·ªÉ may m·∫Øn ho·∫∑c thi√™n v·ªã

## ‚ö†Ô∏è K·∫æT QU·∫¢ T·ª™ NOTEBOOK M·∫™U CHO TH·∫§Y OVERFITTING

### Decision Tree Results:
```
Class 0: precision=1.00, recall=1.00, f1=1.00
Class 1: precision=0.83, recall=0.81, f1=0.82
Overall accuracy: 1.00
```
‚Üí **NGHI NG·ªú OVERFITTING** - Perfect score cho class 0

### Logistic Regression Results:
```
Training accuracy: 0.935
Test accuracy: 0.919
```
‚Üí **Suy gi·∫£m performance** t·ª´ train ‚Üí test

## üîß C√ÅCH PH√ÅT HI·ªÜN OVERFITTING

### 1. **So s√°nh Train vs Test Performance**
```python
# Train accuracy
train_pred = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_pred)

# Test accuracy  
test_pred = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_pred)

print(f"Train: {train_accuracy:.4f}")
print(f"Test: {test_accuracy:.4f}")
print(f"Gap: {train_accuracy - test_accuracy:.4f}")
```

### 2. **S·ª≠ d·ª•ng Metrics ph√π h·ª£p cho Imbalanced Data**
```python
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score

# Precision, Recall, F1 cho class thi·ªÉu s·ªë
precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)
print(f"Fraud Precision: {precision[1]:.4f}")
print(f"Fraud Recall: {recall[1]:.4f}")
print(f"Fraud F1: {f1[1]:.4f}")

# AUC Score
auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
print(f"AUC: {auc:.4f}")
```

### 3. **Learning Curves**
```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5, scoring='f1'
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation F1')
plt.legend()
plt.show()
```

## üõ°Ô∏è GI·∫¢I PH√ÅP CH·ªêNG OVERFITTING

### 1. **Cross-Validation v·ªõi Stratification**
```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf, scoring='f1')
print(f"CV F1 Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
```

### 2. **Regularization**
```python
# Decision Tree v·ªõi constraints
dtree = DecisionTreeClassifier(
    max_depth=10,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

# Logistic Regression v·ªõi regularization
model = LogisticRegression(C=0.1, penalty='l2')
```

### 3. **X·ª≠ l√Ω Imbalanced Data**
```python
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight

# SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Class Weight
class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
model = LogisticRegression(class_weight='balanced')
```

### 4. **Early Stopping & Validation Split**
```python
from sklearn.model_selection import train_test_split

# Chia th√†nh train/val/test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp)

# Monitor validation performance
```

## üéØ METRICS ƒê√öNG CHO FRAUD DETECTION

### Th·ª© t·ª± ∆∞u ti√™n:
1. **Recall (Sensitivity)** - Ph√°t hi·ªán ƒë∆∞·ª£c bao nhi√™u % fraud th·ª±c t·∫ø
2. **Precision** - Trong nh·ªØng d·ª± ƒëo√°n fraud, bao nhi√™u % l√† ƒë√∫ng
3. **F1-Score** - C√¢n b·∫±ng gi·ªØa Precision v√† Recall
4. **AUC-ROC** - Kh·∫£ nƒÉng ph√¢n bi·ªát gi·ªØa fraud v√† non-fraud
5. **Accuracy** - Ch·ªâ tham kh·∫£o, kh√¥ng n√™n l√†m metric ch√≠nh

### Confusion Matrix Analysis:
```
                Predicted
                0    1
Actual    0   TN   FP  ‚Üê Type I Error (False Alarm)
          1   FN   TP  ‚Üê Type II Error (Missed Fraud) - NGUY HI·ªÇM!
```

**Trong fraud detection: Type II Error (FN) nghi√™m tr·ªçng h∆°n Type I Error (FP)**
